use_gpu: True
device: 0
seed: 12345
outdir: exp/fedavg/v5/
federate:
  mode: standalone
  method: FedEM-NLP #fedavg
  local_update_steps: 1
  batch_or_epoch: epoch
  total_round_num: 100
  client_num: 18
  num_grouped_clients: [1, 3, 3, 2, 5, 4]
data:
  type: fednlp_data
  root: /mnt/dongchenhe.dch/datasets/fednlp/v5/
  num_workers: 0
  cache_dir: cache/v5/
model:
  type: fednlp_model
  model_type: google/bert_uncased_L-2_H-128_A-2
  bos_token: '[unused0]'
  eos_token: '[unused1]'
  eoq_token: '[unused2]'
  model_num_per_trainer: 3
personalization:
  local_param: ['classifier', 'encoder.pooler', 'decoder']
  regular_weight: 0.01
trainer:
  type: fednlp_trainer
  disp_freq: 50
  val_freq: 100000000  # eval freq across batches
optimizer:
  type: AdamW
  lr: 5e-4
  weight_decay: 0.01
  grad_clip: 1.0
scheduler:
  type: step
  warmup_ratio: 0.1
eval:
  split: ['test']
  report: ['group_avg']
  freq: 100000000  # eval freq across rounds
