use_gpu: True
device: 0
seed: 12345
outdir: exp/pfednlp_contrast/v5/
federate:
  mode: standalone
  method: pfednlp
  local_update_steps: 1
  batch_or_epoch: epoch
  total_round_num: 100
  client_num: 18
  num_grouped_clients: [1, 3, 3, 2, 5, 4]
aggregator:
  num_agg_topk: [16, 16, 16, 16, 8, 8]
  inside_weight: 1.0
  outside_weight: 0.0
  proto_weight: 0.1
  synth_ratio: 0.5
data:
  type: pfednlp_contrast_data
  root: /mnt/dongchenhe.dch/datasets/fednlp/v5/
  num_workers: 0
  cache_dir: cache/v5/
  num_contrast: 20000
model:
  type: pfednlp_contrast_model
  model_type: google/bert_uncased_L-2_H-128_A-2
  bos_token: '[unused0]'
  eos_token: '[unused1]'
  eoq_token: '[unused2]'
  train_contrast: True
  contrast_topk: 20
  contrast_temp: 1.0
personalization:
  local_param: ['classifier', 'contrast_head', 'encoder.pooler', 'decoder']
trainer:
  type: pfednlp_contrast_trainer
  disp_freq: 50
  val_freq: 100000000  # eval freq across batches
optimizer:
  type: AdamW
  lr: 5e-4
  weight_decay: 0.01
  grad_clip: 1.0
scheduler:
  type: step
  warmup_ratio: 0.1
eval:
  split: ['test']
  report: ['group_avg']
  freq: 100000000  # eval freq across rounds
